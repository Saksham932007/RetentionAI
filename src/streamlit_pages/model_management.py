"""
Model Management page for RetentionAI Streamlit application.

This page provides MLflow model registry interface including model browsing,
promotion status, performance comparisons, model rollback options,
training job triggers, and system health monitoring.
"""

import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from typing import Dict, Any, List, Optional, Tuple
from pathlib import Path
import sys
import logging
from datetime import datetime, timedelta
import json
import subprocess
import time

# Setup path for imports
current_dir = Path(__file__).parent
sys.path.append(str(current_dir.parent))

try:
    from config import MODELS_DIR, EXPERIMENT_NAME
    from database import get_database_manager
    from promote import ModelPromoter, promote_best_model
    from utils import load_json, save_json
    from train import ModelTrainer
except ImportError as e:
    st.error(f"Failed to import required modules: {e}")
    st.stop()

# Optional imports
try:
    import mlflow
    import mlflow.sklearn
    from mlflow.tracking import MlflowClient
    MLFLOW_AVAILABLE = True
except ImportError:
    MLFLOW_AVAILABLE = False

logger = logging.getLogger(__name__)


class ModelManagementPage:
    """Model management and MLflow registry interface for RetentionAI."""
    
    def __init__(self):
        """Initialize the model management page."""
        self.db_manager = get_database_manager()
        self.models_dir = Path(MODELS_DIR)
        self.promoter = ModelPromoter()
        
        # Initialize session state
        if 'training_jobs' not in st.session_state:\n            st.session_state.training_jobs = []\n        if 'system_alerts' not in st.session_state:\n            st.session_state.system_alerts = []\n    \n    @st.cache_data\n    def get_mlflow_experiments(_self) -> List[Dict[str, Any]]:\n        \"\"\"Get MLflow experiments and runs.\"\"\"\n        if not MLFLOW_AVAILABLE:\n            return []\n        \n        try:\n            client = MlflowClient()\n            experiments = client.search_experiments()\n            \n            experiment_data = []\n            for exp in experiments:\n                runs = client.search_runs(\n                    experiment_ids=[exp.experiment_id],\n                    order_by=[\"start_time DESC\"],\n                    max_results=20\n                )\n                \n                experiment_data.append({\n                    'name': exp.name,\n                    'experiment_id': exp.experiment_id,\n                    'runs': runs,\n                    'lifecycle_stage': exp.lifecycle_stage\n                })\n            \n            return experiment_data\n            \n        except Exception as e:\n            logger.error(f\"Error loading MLflow experiments: {e}\")\n            return []\n    \n    @st.cache_data\n    def get_production_models(_self) -> List[Dict[str, Any]]:\n        \"\"\"Get production models from model registry.\"\"\"\n        try:\n            models = _self.promoter.list_production_models()\n            return models\n        except Exception as e:\n            logger.error(f\"Error loading production models: {e}\")\n            return []\n    \n    def render_system_overview(self) -> None:\n        \"\"\"Render system overview and health status.\"\"\"\n        \n        st.markdown(\"### üè• System Health Overview\")\n        \n        col1, col2, col3, col4 = st.columns(4)\n        \n        # Database status\n        with col1:\n            try:\n                result = self.db_manager.execute_query(\"SELECT COUNT(*) as count FROM sqlite_master WHERE type='table'\")\n                table_count = result.iloc[0]['count']\n                if table_count > 0:\n                    st.metric(\"Database\", \"‚úÖ Connected\", f\"{table_count} tables\")\n                else:\n                    st.metric(\"Database\", \"‚ö†Ô∏è Empty\", \"No tables\")\n            except:\n                st.metric(\"Database\", \"‚ùå Error\", \"Connection failed\")\n        \n        # MLflow status\n        with col2:\n            if MLFLOW_AVAILABLE:\n                try:\n                    experiments = self.get_mlflow_experiments()\n                    if experiments:\n                        total_runs = sum(len(exp['runs']) for exp in experiments)\n                        st.metric(\"MLflow\", \"‚úÖ Active\", f\"{total_runs} runs\")\n                    else:\n                        st.metric(\"MLflow\", \"‚ö†Ô∏è No Data\", \"No experiments\")\n                except:\n                    st.metric(\"MLflow\", \"‚ùå Error\", \"Connection failed\")\n            else:\n                st.metric(\"MLflow\", \"‚ùå Unavailable\", \"Not installed\")\n        \n        # Production models\n        with col3:\n            production_models = self.get_production_models()\n            if production_models:\n                current_model = next((m for m in production_models if m['is_current']), None)\n                if current_model:\n                    st.metric(\"Production\", \"‚úÖ Active\", f\"v{current_model['version'][-8:]}\")\n                else:\n                    st.metric(\"Production\", \"‚ö†Ô∏è No Current\", f\"{len(production_models)} models\")\n            else:\n                st.metric(\"Production\", \"‚ùå Empty\", \"No models\")\n        \n        # Training status\n        with col4:\n            training_dirs = list(self.models_dir.glob(\"training_*\"))\n            if training_dirs:\n                latest_training = max(training_dirs, key=lambda x: x.stat().st_mtime)\n                age = datetime.now() - datetime.fromtimestamp(latest_training.stat().st_mtime)\n                if age.days < 1:\n                    st.metric(\"Training\", \"üöÄ Recent\", f\"{age.seconds//3600}h ago\")\n                else:\n                    st.metric(\"Training\", \"üìÖ Stale\", f\"{age.days}d ago\")\n            else:\n                st.metric(\"Training\", \"‚ùå None\", \"No training runs\")\n        \n        # System alerts\n        if st.session_state.system_alerts:\n            st.markdown(\"#### üö® System Alerts\")\n            for alert in st.session_state.system_alerts[-5:]:  # Show last 5 alerts\n                alert_type = alert.get('type', 'info')\n                icon = {'error': '‚ùå', 'warning': '‚ö†Ô∏è', 'info': '‚ÑπÔ∏è', 'success': '‚úÖ'}.get(alert_type, '‚ÑπÔ∏è')\n                st.markdown(f\"{icon} **{alert['timestamp']}**: {alert['message']}\")\n    \n    def render_model_registry(self) -> None:\n        \"\"\"Render model registry interface.\"\"\"\n        \n        st.markdown(\"### üìö Model Registry\")\n        \n        if not MLFLOW_AVAILABLE:\n            st.warning(\"MLflow not available. Showing local models only.\")\n        \n        tab1, tab2 = st.tabs([\"Production Models\", \"Experiment Runs\"])\n        \n        with tab1:\n            self.render_production_models()\n        \n        with tab2:\n            if MLFLOW_AVAILABLE:\n                self.render_experiment_runs()\n            else:\n                st.info(\"MLflow required for experiment tracking. Please install MLflow to view experiment runs.\")\n    \n    def render_production_models(self) -> None:\n        \"\"\"Render production models management.\"\"\"\n        \n        st.markdown(\"#### üöÄ Production Models\")\n        \n        production_models = self.get_production_models()\n        \n        if not production_models:\n            st.info(\"No production models found. Promote a model from experiments to get started.\")\n            return\n        \n        # Models table\n        models_data = []\n        for model in production_models:\n            models_data.append({\n                'Version': model['version'],\n                'Status': 'üü¢ Current' if model['is_current'] else '‚ö™ Previous',\n                'AUC': f\"{model['validation_auc']:.4f}\" if model['validation_auc'] else 'N/A',\n                'Promoted': model['promotion_timestamp'][:10] if model['promotion_timestamp'] else 'Unknown',\n                'Run ID': model['run_id'][:8] if model['run_id'] else 'Unknown',\n                'Path': model['model_path']\n            })\n        \n        models_df = pd.DataFrame(models_data)\n        \n        # Display table with selection\n        selected_model = st.selectbox(\n            \"Select model:\",\n            [f\"{row['Version']} ({row['Status']})\" for _, row in models_df.iterrows()],\n            key=\"production_model_selection\"\n        )\n        \n        st.dataframe(models_df, use_container_width=True, hide_index=True)\n        \n        # Model actions\n        col1, col2, col3, col4 = st.columns(4)\n        \n        with col1:\n            if st.button(\"üîÑ Rollback Model\", key=\"rollback_model\"):\n                if selected_model:\n                    version = selected_model.split(' (')[0]\n                    with st.spinner(f\"Rolling back to {version}...\"):\n                        result = self.promoter.rollback_model(version)\n                        if result['success']:\n                            st.success(f\"‚úÖ Successfully rolled back to {version}\")\n                            st.rerun()\n                        else:\n                            st.error(f\"‚ùå Rollback failed: {result['reason']}\")\n        \n        with col2:\n            if st.button(\"üìä Compare Models\", key=\"compare_models\"):\n                st.info(\"Model comparison feature - coming soon!\")\n        \n        with col3:\n            if st.button(\"üóëÔ∏è Cleanup Old Models\", key=\"cleanup_models\"):\n                with st.spinner(\"Cleaning up old models...\"):\n                    result = self.promoter.cleanup_old_models(keep_versions=3)\n                    st.success(f\"‚úÖ Cleanup complete: {result['message']}\")\n                    if result['cleaned_versions']:\n                        st.info(f\"Removed versions: {', '.join(result['cleaned_versions'])}\")\n        \n        with col4:\n            if st.button(\"‚ÑπÔ∏è Model Details\", key=\"model_details\"):\n                if selected_model:\n                    version = selected_model.split(' (')[0]\n                    model_info = next((m for m in production_models if m['version'] == version), None)\n                    if model_info:\n                        st.json(model_info)\n    \n    def render_experiment_runs(self) -> None:\n        \"\"\"Render MLflow experiment runs.\"\"\"\n        \n        st.markdown(\"#### üß™ Experiment Runs\")\n        \n        experiments = self.get_mlflow_experiments()\n        \n        if not experiments:\n            st.info(\"No MLflow experiments found. Start training to create experiments.\")\n            return\n        \n        # Experiment selection\n        experiment_names = [exp['name'] for exp in experiments]\n        selected_exp_name = st.selectbox(\n            \"Select experiment:\",\n            experiment_names,\n            key=\"experiment_selection\"\n        )\n        \n        selected_experiment = next(\n            (exp for exp in experiments if exp['name'] == selected_exp_name),\n            None\n        )\n        \n        if not selected_experiment:\n            return\n        \n        runs = selected_experiment['runs']\n        \n        if not runs:\n            st.info(f\"No runs found in experiment '{selected_exp_name}'\")\n            return\n        \n        # Runs table\n        runs_data = []\n        for run in runs:\n            metrics = run.data.metrics\n            params = run.data.params\n            \n            runs_data.append({\n                'Run ID': run.info.run_id[:8],\n                'Status': run.info.status,\n                'Start Time': datetime.fromtimestamp(run.info.start_time / 1000).strftime('%Y-%m-%d %H:%M'),\n                'AUC': f\"{metrics.get('val_auc', 0):.4f}\" if 'val_auc' in metrics else 'N/A',\n                'Accuracy': f\"{metrics.get('val_accuracy', 0):.4f}\" if 'val_accuracy' in metrics else 'N/A',\n                'Algorithm': params.get('algorithm', 'Unknown'),\n                'Full Run ID': run.info.run_id\n            })\n        \n        runs_df = pd.DataFrame(runs_data)\n        \n        # Display runs\n        st.dataframe(runs_df.drop('Full Run ID', axis=1), use_container_width=True, hide_index=True)\n        \n        # Run actions\n        selected_run = st.selectbox(\n            \"Select run for actions:\",\n            [f\"{row['Run ID']} (AUC: {row['AUC']})\" for _, row in runs_df.iterrows()],\n            key=\"run_selection\"\n        )\n        \n        col1, col2, col3 = st.columns(3)\n        \n        with col1:\n            if st.button(\"üöÄ Promote to Production\", key=\"promote_run\"):\n                if selected_run:\n                    run_id_short = selected_run.split(' (')[0]\n                    run_row = runs_df[runs_df['Run ID'] == run_id_short].iloc[0]\n                    full_run_id = run_row['Full Run ID']\n                    \n                    with st.spinner(\"Promoting model to production...\"):\n                        # Create mock model info for promotion\n                        model_info = {\n                            'run_id': full_run_id,\n                            'experiment_id': selected_experiment['experiment_id'],\n                            'experiment_name': selected_exp_name,\n                            'metric_name': 'val_auc',\n                            'metric_value': float(run_row['AUC']) if run_row['AUC'] != 'N/A' else 0.5,\n                            'run_name': f\"Run {run_id_short}\",\n                            'start_time': runs[0].info.start_time,\n                            'params': runs[0].data.params,\n                            'metrics': runs[0].data.metrics,\n                            'tags': runs[0].data.tags,\n                            'artifacts': []\n                        }\n                        \n                        # Mock validation (in real implementation, this would validate properly)\n                        validation_results = {\n                            'model_id': full_run_id,\n                            'passed': True,\n                            'metrics': {'auc': float(run_row['AUC']) if run_row['AUC'] != 'N/A' else 0.5},\n                            'errors': []\n                        }\n                        \n                        try:\n                            promotion_result = self.promoter.promote_model(\n                                model_info, validation_results, force=True\n                            )\n                            \n                            if promotion_result['success']:\n                                st.success(f\"‚úÖ Model promoted successfully as {promotion_result['version']}\")\n                                # Add alert\n                                alert = {\n                                    'type': 'success',\n                                    'timestamp': datetime.now().strftime('%H:%M:%S'),\n                                    'message': f\"Model {run_id_short} promoted to production\"\n                                }\n                                st.session_state.system_alerts.append(alert)\n                                st.rerun()\n                            else:\n                                st.error(f\"‚ùå Promotion failed: {promotion_result.get('reason', 'Unknown error')}\")\n                        except Exception as e:\n                            st.error(f\"‚ùå Promotion error: {str(e)}\")\n        \n        with col2:\n            if st.button(\"üìä View Metrics\", key=\"view_metrics\"):\n                if selected_run:\n                    run_id_short = selected_run.split(' (')[0]\n                    run_row = runs_df[runs_df['Run ID'] == run_id_short].iloc[0]\n                    full_run_id = run_row['Full Run ID']\n                    \n                    # Find the run details\n                    run_details = next(\n                        (run for run in runs if run.info.run_id == full_run_id),\n                        None\n                    )\n                    \n                    if run_details:\n                        st.markdown(\"**Metrics:**\")\n                        st.json(dict(run_details.data.metrics))\n                        \n                        st.markdown(\"**Parameters:**\")\n                        st.json(dict(run_details.data.params))\n        \n        with col3:\n            if st.button(\"üîç Run Details\", key=\"run_details\"):\n                if selected_run:\n                    run_id_short = selected_run.split(' (')[0]\n                    run_row = runs_df[runs_df['Run ID'] == run_id_short].iloc[0]\n                    st.info(f\"Run ID: {run_row['Full Run ID']}\")\n    \n    def render_training_controls(self) -> None:\n        \"\"\"Render training job controls.\"\"\"\n        \n        st.markdown(\"### üéØ Training Controls\")\n        \n        col1, col2 = st.columns(2)\n        \n        with col1:\n            st.markdown(\"#### üöÄ Start New Training\")\n            \n            with st.form(\"training_form\"):\n                training_name = st.text_input(\n                    \"Training Job Name\",\n                    value=f\"training_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n                )\n                \n                optimize_hyperparameters = st.checkbox(\n                    \"Optimize Hyperparameters\",\n                    value=True,\n                    help=\"Run hyperparameter optimization (takes longer)\"\n                )\n                \n                n_trials = st.slider(\n                    \"Optimization Trials\",\n                    min_value=10, max_value=100, value=50,\n                    disabled=not optimize_hyperparameters\n                )\n                \n                handle_imbalance = st.checkbox(\n                    \"Handle Class Imbalance\",\n                    value=True,\n                    help=\"Use SMOTE for class imbalance\"\n                )\n                \n                generate_shap = st.checkbox(\n                    \"Generate SHAP Analysis\",\n                    value=False,\n                    help=\"Generate model interpretability (slower)\"\n                )\n                \n                submitted = st.form_submit_button(\"üöÄ Start Training\", type=\"primary\")\n                \n                if submitted:\n                    job_config = {\n                        'name': training_name,\n                        'optimize_hyperparameters': optimize_hyperparameters,\n                        'n_trials': n_trials,\n                        'handle_imbalance': handle_imbalance,\n                        'generate_shap': generate_shap,\n                        'start_time': datetime.now().isoformat(),\n                        'status': 'queued'\n                    }\n                    \n                    # Add to training jobs (in real implementation, this would start actual training)\n                    st.session_state.training_jobs.append(job_config)\n                    \n                    st.success(f\"‚úÖ Training job '{training_name}' queued successfully!\")\n                    st.info(\"‚ÑπÔ∏è In production, this would start a background training process.\")\n                    \n                    # Add alert\n                    alert = {\n                        'type': 'info',\n                        'timestamp': datetime.now().strftime('%H:%M:%S'),\n                        'message': f\"Training job '{training_name}' started\"\n                    }\n                    st.session_state.system_alerts.append(alert)\n        \n        with col2:\n            st.markdown(\"#### üìã Training Queue\")\n            \n            if not st.session_state.training_jobs:\n                st.info(\"No training jobs in queue\")\n            else:\n                for i, job in enumerate(st.session_state.training_jobs[-5:]):  # Show last 5 jobs\n                    status_icon = {\n                        'queued': 'üü°',\n                        'running': 'üîµ',\n                        'completed': 'üü¢',\n                        'failed': 'üî¥'\n                    }.get(job['status'], '‚ö™')\n                    \n                    st.markdown(f\"{status_icon} **{job['name']}** - {job['status'].title()}\")\n                    st.caption(f\"Started: {job['start_time'][:16]}\")\n                    \n                    if st.button(f\"Cancel Job {i}\", key=f\"cancel_job_{i}\", disabled=job['status'] not in ['queued', 'running']):\n                        job['status'] = 'cancelled'\n                        st.rerun()\n                \n                if st.button(\"üóëÔ∏è Clear Completed Jobs\", key=\"clear_jobs\"):\n                    st.session_state.training_jobs = [\n                        job for job in st.session_state.training_jobs \n                        if job['status'] not in ['completed', 'failed', 'cancelled']\n                    ]\n                    st.rerun()\n    \n    def render_auto_promotion(self) -> None:\n        \"\"\"Render auto-promotion controls.\"\"\"\n        \n        st.markdown(\"### ü§ñ Auto-Promotion\")\n        \n        col1, col2 = st.columns(2)\n        \n        with col1:\n            st.markdown(\"#### üìä Promotion Criteria\")\n            \n            min_auc = st.slider(\n                \"Minimum AUC for Promotion\",\n                min_value=0.5, max_value=1.0, value=0.75, step=0.01,\n                help=\"Models below this AUC will not be auto-promoted\"\n            )\n            \n            auto_promote = st.checkbox(\n                \"Enable Auto-Promotion\",\n                value=False,\n                help=\"Automatically promote models that meet criteria\"\n            )\n            \n            promotion_schedule = st.selectbox(\n                \"Promotion Schedule\",\n                [\"After each training\", \"Daily\", \"Weekly\", \"Manual only\"],\n                disabled=not auto_promote\n            )\n        \n        with col2:\n            st.markdown(\"#### üéØ Promotion Actions\")\n            \n            if st.button(\"üîç Find Best Model\", type=\"primary\"):\n                with st.spinner(\"Searching for best model...\"):\n                    try:\n                        result = promote_best_model(\n                            experiment_name=EXPERIMENT_NAME,\n                            min_auc=min_auc,\n                            force=False\n                        )\n                        \n                        if result['success']:\n                            model_auc = result['validation_results']['metrics']['auc']\n                            version = result['promotion_results']['version']\n                            st.success(f\"‚úÖ Best model found and promoted: {version} (AUC: {model_auc:.4f})\")\n                            \n                            # Add alert\n                            alert = {\n                                'type': 'success',\n                                'timestamp': datetime.now().strftime('%H:%M:%S'),\n                                'message': f\"Auto-promoted model {version} (AUC: {model_auc:.3f})\"\n                            }\n                            st.session_state.system_alerts.append(alert)\n                        else:\n                            st.warning(f\"‚ö†Ô∏è No model meets promotion criteria: {result['reason']}\")\n                    \n                    except Exception as e:\n                        st.error(f\"‚ùå Auto-promotion failed: {str(e)}\")\n            \n            if st.button(\"üìà Promotion History\"):\n                production_models = self.get_production_models()\n                if production_models:\n                    # Create promotion timeline\n                    timeline_data = []\n                    for model in sorted(production_models, key=lambda x: x['promotion_timestamp'] or ''):\n                        timeline_data.append({\n                            'Version': model['version'],\n                            'Promoted': model['promotion_timestamp'][:16] if model['promotion_timestamp'] else 'Unknown',\n                            'AUC': f\"{model['validation_auc']:.4f}\" if model['validation_auc'] else 'N/A',\n                            'Status': 'Current' if model['is_current'] else 'Previous'\n                        })\n                    \n                    st.dataframe(pd.DataFrame(timeline_data), use_container_width=True, hide_index=True)\n                else:\n                    st.info(\"No promotion history available\")\n    \n    def render_monitoring_dashboard(self) -> None:\n        \"\"\"Render monitoring dashboard.\"\"\"\n        \n        st.markdown(\"### üìä Model Monitoring\")\n        \n        # Generate sample monitoring data\n        dates = pd.date_range(start=datetime.now() - timedelta(days=30), end=datetime.now(), freq='D')\n        \n        monitoring_data = {\n            'Date': dates,\n            'Prediction_Count': np.random.poisson(100, len(dates)),\n            'Avg_Churn_Probability': np.random.uniform(0.2, 0.3, len(dates)),\n            'Model_Accuracy': 0.85 + np.random.normal(0, 0.02, len(dates)),\n            'Response_Time_ms': np.random.gamma(2, 50, len(dates))\n        }\n        \n        monitoring_df = pd.DataFrame(monitoring_data)\n        \n        # Monitoring visualizations\n        col1, col2 = st.columns(2)\n        \n        with col1:\n            fig_predictions = px.line(\n                monitoring_df, x='Date', y='Prediction_Count',\n                title='Daily Prediction Volume',\n                labels={'Prediction_Count': 'Number of Predictions'}\n            )\n            st.plotly_chart(fig_predictions, use_container_width=True)\n        \n        with col2:\n            fig_accuracy = px.line(\n                monitoring_df, x='Date', y='Model_Accuracy',\n                title='Model Accuracy Over Time',\n                labels={'Model_Accuracy': 'Accuracy'}\n            )\n            st.plotly_chart(fig_accuracy, use_container_width=True)\n        \n        # Performance metrics\n        st.markdown(\"#### üìà Performance Metrics (Last 30 Days)\")\n        \n        col1, col2, col3, col4 = st.columns(4)\n        \n        with col1:\n            total_predictions = monitoring_df['Prediction_Count'].sum()\n            st.metric(\"Total Predictions\", f\"{total_predictions:,}\")\n        \n        with col2:\n            avg_churn_rate = monitoring_df['Avg_Churn_Probability'].mean()\n            st.metric(\"Avg Churn Rate\", f\"{avg_churn_rate:.1%}\")\n        \n        with col3:\n            avg_accuracy = monitoring_df['Model_Accuracy'].mean()\n            st.metric(\"Avg Accuracy\", f\"{avg_accuracy:.3f}\")\n        \n        with col4:\n            avg_response_time = monitoring_df['Response_Time_ms'].mean()\n            st.metric(\"Avg Response Time\", f\"{avg_response_time:.0f}ms\")\n        \n        # Alerts and notifications\n        st.markdown(\"#### üö® Model Alerts\")\n        \n        # Check for alerts\n        alerts = []\n        \n        if avg_accuracy < 0.8:\n            alerts.append(\"‚ö†Ô∏è Model accuracy below threshold (80%)\")\n        \n        if avg_response_time > 1000:\n            alerts.append(\"üêå Average response time above 1000ms\")\n        \n        recent_accuracy = monitoring_df['Model_Accuracy'].tail(7).mean()\n        older_accuracy = monitoring_df['Model_Accuracy'].head(7).mean()\n        if recent_accuracy < older_accuracy - 0.05:\n            alerts.append(\"üìâ Model accuracy declining over past week\")\n        \n        if alerts:\n            for alert in alerts:\n                st.warning(alert)\n        else:\n            st.success(\"‚úÖ All monitoring metrics are within normal ranges\")\n    \n    def render_page(self) -> None:\n        \"\"\"Render the complete model management page.\"\"\"\n        \n        # System overview\n        self.render_system_overview()\n        \n        st.markdown(\"---\")\n        \n        # Model registry\n        self.render_model_registry()\n        \n        st.markdown(\"---\")\n        \n        # Training controls\n        self.render_training_controls()\n        \n        st.markdown(\"---\")\n        \n        # Auto-promotion\n        self.render_auto_promotion()\n        \n        st.markdown(\"---\")\n        \n        # Monitoring dashboard\n        self.render_monitoring_dashboard()\n\n\ndef render_model_management_page():\n    \"\"\"Main function to render model management page.\"\"\"\n    page = ModelManagementPage()\n    page.render_page()\n\n\nif __name__ == \"__main__\":\n    render_model_management_page()