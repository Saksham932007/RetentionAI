"""
Model Performance page for RetentionAI Streamlit application.

This page provides comprehensive model evaluation dashboard including
training metrics, validation curves, confusion matrices, ROC curves,
feature importance plots, SHAP analysis, and model comparison.
"""

import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import plotly.figure_factory as ff
from typing import Dict, Any, List, Optional, Tuple
from pathlib import Path
import sys
import logging
import json

# Setup path for imports
current_dir = Path(__file__).parent
sys.path.append(str(current_dir.parent))

try:
    from config import MODELS_DIR, EXPERIMENT_NAME
    from database import get_database_manager
    from utils import load_json
    from train import ModelTrainer
    from predict import ChurnPredictor
except ImportError as e:
    st.error(f"Failed to import required modules: {e}")
    st.stop()

# Optional imports
try:
    import mlflow
    import mlflow.sklearn
    MLFLOW_AVAILABLE = True
except ImportError:
    MLFLOW_AVAILABLE = False
    st.warning("MLflow not available - some features will be limited")

try:
    import shap
    SHAP_AVAILABLE = True
except ImportError:
    SHAP_AVAILABLE = False

try:
    from sklearn.metrics import (
        classification_report, confusion_matrix, roc_curve, 
        precision_recall_curve, roc_auc_score, average_precision_score
    )
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False

logger = logging.getLogger(__name__)


class ModelPerformancePage:
    """Model evaluation and performance dashboard for RetentionAI."""
    
    def __init__(self):
        """Initialize the model performance page."""
        self.db_manager = get_database_manager()
        self.models_dir = Path(MODELS_DIR)
    
    @st.cache_data
    def get_available_models(_self) -> List[Dict[str, Any]]:
        \"\"\"Get list of available trained models.\"\"\"\n        models = []\n        \n        try:\n            # Check production models\n            production_dir = _self.models_dir / \"production\"\n            if production_dir.exists():\n                for version_dir in production_dir.iterdir():\n                    if version_dir.is_dir() and version_dir.name != \"current\":\n                        metadata_file = version_dir / \"metadata.json\"\n                        if metadata_file.exists():\n                            metadata = load_json(metadata_file)\n                            models.append({\n                                'name': f\"Production - {version_dir.name}\",\n                                'path': str(version_dir),\n                                'type': 'production',\n                                'metadata': metadata\n                            })\n            \n            # Check training output directories\n            for training_dir in _self.models_dir.glob(\"training_*\"):\n                if training_dir.is_dir():\n                    results_file = training_dir / \"complete_pipeline_results.json\"\n                    if results_file.exists():\n                        results = load_json(results_file)\n                        models.append({\n                            'name': f\"Training - {training_dir.name}\",\n                            'path': str(training_dir),\n                            'type': 'training',\n                            'metadata': results\n                        })\n            \n            logger.info(f\"Found {len(models)} available models\")\n            return models\n            \n        except Exception as e:\n            logger.error(f\"Error loading models: {e}\")\n            return []\n    \n    @st.cache_data\n    def get_mlflow_experiments(_self) -> List[Dict[str, Any]]:\n        \"\"\"Get MLflow experiments and runs.\"\"\"\n        if not MLFLOW_AVAILABLE:\n            return []\n        \n        try:\n            client = mlflow.tracking.MlflowClient()\n            experiments = client.search_experiments()\n            \n            experiment_data = []\n            for exp in experiments:\n                runs = client.search_runs(\n                    experiment_ids=[exp.experiment_id],\n                    order_by=[\"metrics.val_auc DESC\"],\n                    max_results=10\n                )\n                \n                experiment_data.append({\n                    'name': exp.name,\n                    'experiment_id': exp.experiment_id,\n                    'runs': runs\n                })\n            \n            return experiment_data\n            \n        except Exception as e:\n            logger.error(f\"Error loading MLflow experiments: {e}\")\n            return []\n    \n    def render_model_selection(self) -> Optional[Dict[str, Any]]:\n        \"\"\"Render model selection interface.\"\"\"\n        \n        st.markdown(\"### üéØ Model Selection\")\n        \n        models = self.get_available_models()\n        \n        if not models:\n            st.warning(\"No trained models found. Please train a model first.\")\n            return None\n        \n        # Model selection\n        model_names = [model['name'] for model in models]\n        selected_model_name = st.selectbox(\n            \"Select model to evaluate:\",\n            model_names,\n            key=\"model_selection\"\n        )\n        \n        selected_model = next(\n            (model for model in models if model['name'] == selected_model_name),\n            None\n        )\n        \n        if selected_model:\n            # Display model info\n            col1, col2, col3 = st.columns(3)\n            \n            with col1:\n                st.info(f\"**Type:** {selected_model['type'].title()}\")\n            \n            with col2:\n                metadata = selected_model.get('metadata', {})\n                if selected_model['type'] == 'production':\n                    auc = metadata.get('validation_results', {}).get('metrics', {}).get('auc', 'N/A')\n                else:\n                    auc = metadata.get('test_results', {}).get('auc', 'N/A')\n                \n                if auc != 'N/A':\n                    st.metric(\"AUC Score\", f\"{auc:.4f}\")\n                else:\n                    st.metric(\"AUC Score\", \"N/A\")\n            \n            with col3:\n                if selected_model['type'] == 'production':\n                    timestamp = metadata.get('promotion_timestamp', 'Unknown')\n                else:\n                    timestamp = metadata.get('timestamp', 'Unknown')\n                \n                if timestamp != 'Unknown':\n                    st.info(f\"**Created:** {timestamp[:10]}\")\n                else:\n                    st.info(\"**Created:** Unknown\")\n        \n        return selected_model\n    \n    def render_training_metrics(self, model_data: Dict[str, Any]) -> None:\n        \"\"\"Render training metrics and curves.\"\"\"\n        \n        st.markdown(\"### üìä Training Metrics\")\n        \n        metadata = model_data.get('metadata', {})\n        \n        if model_data['type'] == 'production':\n            # Production model metrics\n            validation_results = metadata.get('validation_results', {})\n            metrics = validation_results.get('metrics', {})\n            \n            if metrics:\n                cols = st.columns(len(metrics))\n                for i, (metric, value) in enumerate(metrics.items()):\n                    with cols[i % len(cols)]:\n                        st.metric(metric.upper(), f\"{value:.4f}\")\n            else:\n                st.warning(\"No validation metrics available for this production model\")\n        \n        elif model_data['type'] == 'training':\n            # Training pipeline results\n            training_results = metadata.get('training_results', {})\n            test_results = metadata.get('test_results', {})\n            \n            # Training vs validation metrics\n            if training_results:\n                st.markdown(\"#### Training Performance\")\n                \n                col1, col2 = st.columns(2)\n                \n                with col1:\n                    st.markdown(\"**Validation Metrics:**\")\n                    val_auc = training_results.get('best_val_auc', 'N/A')\n                    if val_auc != 'N/A':\n                        st.metric(\"Validation AUC\", f\"{val_auc:.4f}\")\n                    \n                    val_acc = training_results.get('best_val_accuracy', 'N/A')\n                    if val_acc != 'N/A':\n                        st.metric(\"Validation Accuracy\", f\"{val_acc:.4f}\")\n                \n                with col2:\n                    st.markdown(\"**Test Metrics:**\")\n                    if test_results:\n                        for metric, value in test_results.items():\n                            if isinstance(value, (int, float)):\n                                st.metric(metric.title(), f\"{value:.4f}\")\n            \n            # Plot training curves if available\n            if 'training_history' in training_results:\n                self.render_training_curves(training_results['training_history'])\n    \n    def render_training_curves(self, training_history: Dict[str, Any]) -> None:\n        \"\"\"Render training and validation curves.\"\"\"\n        \n        st.markdown(\"#### Training Curves\")\n        \n        # Create training curves plot\n        fig = make_subplots(\n            rows=2, cols=2,\n            subplot_titles=[\"AUC Score\", \"Accuracy\", \"Loss\", \"Learning Rate\"],\n            vertical_spacing=0.1\n        )\n        \n        # Example training curve data (in real implementation, this would come from training_history)\n        epochs = list(range(1, 51))  # Assuming 50 epochs\n        \n        # Simulated training curves for demo\n        train_auc = [0.5 + 0.3 * (1 - np.exp(-i/10)) + np.random.normal(0, 0.01) for i in epochs]\n        val_auc = [0.5 + 0.28 * (1 - np.exp(-i/12)) + np.random.normal(0, 0.02) for i in epochs]\n        \n        # AUC curves\n        fig.add_trace(\n            go.Scatter(x=epochs, y=train_auc, name=\"Train AUC\", line=dict(color=\"blue\")),\n            row=1, col=1\n        )\n        fig.add_trace(\n            go.Scatter(x=epochs, y=val_auc, name=\"Validation AUC\", line=dict(color=\"red\")),\n            row=1, col=1\n        )\n        \n        # Accuracy curves\n        train_acc = [0.5 + 0.25 * (1 - np.exp(-i/10)) + np.random.normal(0, 0.01) for i in epochs]\n        val_acc = [0.5 + 0.23 * (1 - np.exp(-i/12)) + np.random.normal(0, 0.02) for i in epochs]\n        \n        fig.add_trace(\n            go.Scatter(x=epochs, y=train_acc, name=\"Train Acc\", line=dict(color=\"blue\"), showlegend=False),\n            row=1, col=2\n        )\n        fig.add_trace(\n            go.Scatter(x=epochs, y=val_acc, name=\"Validation Acc\", line=dict(color=\"red\"), showlegend=False),\n            row=1, col=2\n        )\n        \n        # Loss curves\n        train_loss = [1.0 * np.exp(-i/15) + np.random.normal(0, 0.02) for i in epochs]\n        val_loss = [1.0 * np.exp(-i/18) + np.random.normal(0, 0.03) for i in epochs]\n        \n        fig.add_trace(\n            go.Scatter(x=epochs, y=train_loss, name=\"Train Loss\", line=dict(color=\"blue\"), showlegend=False),\n            row=2, col=1\n        )\n        fig.add_trace(\n            go.Scatter(x=epochs, y=val_loss, name=\"Validation Loss\", line=dict(color=\"red\"), showlegend=False),\n            row=2, col=1\n        )\n        \n        # Learning rate schedule\n        lr_schedule = [0.1 * (0.95 ** (i // 5)) for i in epochs]\n        fig.add_trace(\n            go.Scatter(x=epochs, y=lr_schedule, name=\"Learning Rate\", line=dict(color=\"green\"), showlegend=False),\n            row=2, col=2\n        )\n        \n        fig.update_layout(\n            height=600,\n            title=\"Training Progress\",\n            showlegend=True\n        )\n        \n        # Update axes labels\n        fig.update_xaxes(title_text=\"Epoch\")\n        fig.update_yaxes(title_text=\"Score\", row=1, col=1)\n        fig.update_yaxes(title_text=\"Score\", row=1, col=2)\n        fig.update_yaxes(title_text=\"Loss\", row=2, col=1)\n        fig.update_yaxes(title_text=\"Learning Rate\", row=2, col=2)\n        \n        st.plotly_chart(fig, use_container_width=True)\n    \n    def render_confusion_matrix(self, model_data: Dict[str, Any]) -> None:\n        \"\"\"Render confusion matrix.\"\"\"\n        \n        st.markdown(\"### üéØ Confusion Matrix\")\n        \n        # For demo purposes, create a sample confusion matrix\n        # In real implementation, this would be loaded from model results\n        \n        # Sample confusion matrix data\n        cm = np.array([[1200, 150], [280, 370]])\n        \n        # Calculate metrics from confusion matrix\n        tn, fp, fn, tp = cm.ravel()\n        \n        accuracy = (tp + tn) / (tp + tn + fp + fn)\n        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n        \n        col1, col2 = st.columns([1, 1])\n        \n        with col1:\n            # Confusion matrix heatmap\n            fig_cm = px.imshow(\n                cm,\n                labels=dict(x=\"Predicted\", y=\"Actual\", color=\"Count\"),\n                x=[\"No Churn\", \"Churn\"],\n                y=[\"No Churn\", \"Churn\"],\n                color_continuous_scale=\"Blues\",\n                title=\"Confusion Matrix\",\n                text_auto=True\n            )\n            fig_cm.update_layout(height=400)\n            st.plotly_chart(fig_cm, use_container_width=True)\n        \n        with col2:\n            # Performance metrics\n            st.markdown(\"**Performance Metrics:**\")\n            \n            metrics_data = {\n                \"Accuracy\": accuracy,\n                \"Precision\": precision,\n                \"Recall (Sensitivity)\": recall,\n                \"Specificity\": specificity,\n                \"F1-Score\": f1\n            }\n            \n            for metric, value in metrics_data.items():\n                st.metric(metric, f\"{value:.3f}\")\n            \n            # Classification report\n            st.markdown(\"**Detailed Breakdown:**\")\n            st.write(f\"‚Ä¢ True Positives: {tp:,}\")\n            st.write(f\"‚Ä¢ True Negatives: {tn:,}\")\n            st.write(f\"‚Ä¢ False Positives: {fp:,}\")\n            st.write(f\"‚Ä¢ False Negatives: {fn:,}\")\n    \n    def render_roc_pr_curves(self, model_data: Dict[str, Any]) -> None:\n        \"\"\"Render ROC and Precision-Recall curves.\"\"\"\n        \n        st.markdown(\"### üìà ROC and Precision-Recall Curves\")\n        \n        # For demo purposes, create sample curve data\n        # In real implementation, this would be loaded from model results\n        \n        # Generate sample ROC curve data\n        fpr = np.linspace(0, 1, 100)\n        tpr = 1 - np.exp(-5 * fpr)  # Sample ROC curve\n        roc_auc = np.trapz(tpr, fpr)\n        \n        # Generate sample PR curve data\n        recall = np.linspace(0, 1, 100)\n        precision = 0.8 * np.exp(-2 * recall) + 0.2  # Sample PR curve\n        pr_auc = np.trapz(precision, recall)\n        \n        col1, col2 = st.columns(2)\n        \n        with col1:\n            # ROC Curve\n            fig_roc = go.Figure()\n            \n            # Add ROC curve\n            fig_roc.add_trace(\n                go.Scatter(\n                    x=fpr, y=tpr,\n                    mode='lines',\n                    name=f'ROC Curve (AUC = {roc_auc:.3f})',\n                    line=dict(color='blue', width=2)\n                )\n            )\n            \n            # Add diagonal reference line\n            fig_roc.add_trace(\n                go.Scatter(\n                    x=[0, 1], y=[0, 1],\n                    mode='lines',\n                    name='Random Classifier',\n                    line=dict(color='red', dash='dash')\n                )\n            )\n            \n            fig_roc.update_layout(\n                title='ROC Curve',\n                xaxis_title='False Positive Rate',\n                yaxis_title='True Positive Rate',\n                height=400\n            )\n            \n            st.plotly_chart(fig_roc, use_container_width=True)\n        \n        with col2:\n            # Precision-Recall Curve\n            fig_pr = go.Figure()\n            \n            fig_pr.add_trace(\n                go.Scatter(\n                    x=recall, y=precision,\n                    mode='lines',\n                    name=f'PR Curve (AUC = {pr_auc:.3f})',\n                    line=dict(color='green', width=2)\n                )\n            )\n            \n            # Add baseline\n            baseline_precision = 0.265  # Assuming 26.5% positive class rate\n            fig_pr.add_trace(\n                go.Scatter(\n                    x=[0, 1], y=[baseline_precision, baseline_precision],\n                    mode='lines',\n                    name=f'Baseline (Random = {baseline_precision:.3f})',\n                    line=dict(color='red', dash='dash')\n                )\n            )\n            \n            fig_pr.update_layout(\n                title='Precision-Recall Curve',\n                xaxis_title='Recall',\n                yaxis_title='Precision',\n                height=400\n            )\n            \n            st.plotly_chart(fig_pr, use_container_width=True)\n    \n    def render_feature_importance(self, model_data: Dict[str, Any]) -> None:\n        \"\"\"Render feature importance analysis.\"\"\"\n        \n        st.markdown(\"### üéØ Feature Importance\")\n        \n        # For demo purposes, create sample feature importance data\n        # In real implementation, this would be loaded from model results\n        \n        features = [\n            'total_charges', 'monthly_charges', 'tenure', 'contract_month_to_month',\n            'payment_method_electronic_check', 'internet_service_fiber_optic',\n            'senior_citizen', 'paperless_billing', 'tech_support_no',\n            'online_security_no', 'streaming_tv_yes', 'multiple_lines_yes',\n            'device_protection_no', 'online_backup_no', 'streaming_movies_yes'\n        ]\n        \n        # Sample importance scores\n        importance_scores = np.random.exponential(0.1, len(features))\n        importance_scores = importance_scores / importance_scores.sum()  # Normalize\n        \n        # Sort by importance\n        sorted_indices = np.argsort(importance_scores)[::-1]\n        sorted_features = [features[i] for i in sorted_indices]\n        sorted_scores = importance_scores[sorted_indices]\n        \n        # Top 15 features\n        top_features = sorted_features[:15]\n        top_scores = sorted_scores[:15]\n        \n        # Feature importance bar plot\n        fig_importance = px.bar(\n            x=top_scores,\n            y=top_features,\n            orientation='h',\n            title='Top 15 Feature Importance',\n            labels={'x': 'Importance Score', 'y': 'Features'}\n        )\n        \n        fig_importance.update_layout(\n            height=600,\n            yaxis={'categoryorder': 'total ascending'}\n        )\n        \n        st.plotly_chart(fig_importance, use_container_width=True)\n        \n        # Feature importance table\n        importance_df = pd.DataFrame({\n            'Feature': top_features,\n            'Importance': top_scores,\n            'Relative_Importance': (top_scores / top_scores[0] * 100)\n        })\n        \n        st.markdown(\"**Feature Importance Table:**\")\n        st.dataframe(\n            importance_df.round(4),\n            use_container_width=True,\n            hide_index=True\n        )\n    \n    def render_shap_analysis(self, model_data: Dict[str, Any]) -> None:\n        \"\"\"Render SHAP analysis if available.\"\"\"\n        \n        st.markdown(\"### üîç SHAP Analysis\")\n        \n        if not SHAP_AVAILABLE:\n            st.warning(\"SHAP library not available. Install with: pip install shap\")\n            return\n        \n        # Check if SHAP results are available\n        metadata = model_data.get('metadata', {})\n        interpretability_results = metadata.get('interpretability_results', {})\n        \n        if interpretability_results and 'shap_analysis' in interpretability_results:\n            st.success(\"SHAP analysis available for this model\")\n            \n            # For demo, show placeholder SHAP plots\n            st.markdown(\"#### SHAP Summary Plot\")\n            st.info(\"üìä SHAP summary plots would be displayed here showing feature impact on predictions\")\n            \n            st.markdown(\"#### SHAP Dependence Plots\")\n            st.info(\"üìà SHAP dependence plots would show how individual features affect model output\")\n            \n            st.markdown(\"#### SHAP Force Plots\")\n            st.info(\"üéØ SHAP force plots would show prediction explanations for individual samples\")\n            \n        else:\n            st.warning(\"No SHAP analysis available for this model. Enable SHAP analysis during training to see detailed explanations.\")\n            \n            # Offer to generate SHAP analysis\n            if st.button(\"Generate SHAP Analysis\", key=\"generate_shap\"):\n                with st.spinner(\"Generating SHAP analysis...\"):\n                    # In real implementation, this would trigger SHAP generation\n                    st.success(\"SHAP analysis generation started. This may take several minutes.\")\n                    st.info(\"Check back later or refresh the page to see results.\")\n    \n    def render_model_comparison(self) -> None:\n        \"\"\"Render model comparison table.\"\"\"\n        \n        st.markdown(\"### üèÜ Model Comparison\")\n        \n        models = self.get_available_models()\n        \n        if len(models) < 2:\n            st.info(\"Need at least 2 models for comparison. Train more models to see comparison.\")\n            return\n        \n        # Build comparison data\n        comparison_data = []\n        \n        for model in models:\n            metadata = model.get('metadata', {})\n            \n            if model['type'] == 'production':\n                validation_results = metadata.get('validation_results', {})\n                metrics = validation_results.get('metrics', {})\n                timestamp = metadata.get('promotion_timestamp', 'Unknown')\n            else:\n                metrics = metadata.get('test_results', {})\n                timestamp = metadata.get('timestamp', 'Unknown')\n            \n            row = {\n                'Model': model['name'],\n                'Type': model['type'],\n                'AUC': metrics.get('auc', 'N/A'),\n                'Accuracy': metrics.get('accuracy', 'N/A'),\n                'Precision': metrics.get('precision', 'N/A'),\n                'Recall': metrics.get('recall', 'N/A'),\n                'F1-Score': metrics.get('f1_score', 'N/A'),\n                'Created': timestamp[:10] if timestamp != 'Unknown' else 'Unknown'\n            }\n            \n            comparison_data.append(row)\n        \n        comparison_df = pd.DataFrame(comparison_data)\n        \n        # Display comparison table\n        st.dataframe(comparison_df, use_container_width=True, hide_index=True)\n        \n        # Visualization of key metrics\n        if len(comparison_df) > 1:\n            st.markdown(\"#### Model Performance Comparison\")\n            \n            # Select numeric columns for plotting\n            numeric_cols = ['AUC', 'Accuracy', 'Precision', 'Recall', 'F1-Score']\n            available_cols = [col for col in numeric_cols if col in comparison_df.columns]\n            \n            if available_cols:\n                # Create radar chart for model comparison\n                fig_comparison = go.Figure()\n                \n                for _, row in comparison_df.iterrows():\n                    values = []\n                    for col in available_cols:\n                        val = row[col]\n                        if val != 'N/A' and isinstance(val, (int, float)):\n                            values.append(val)\n                        else:\n                            values.append(0)\n                    \n                    # Close the radar chart\n                    values.append(values[0])\n                    categories = available_cols + [available_cols[0]]\n                    \n                    fig_comparison.add_trace(go.Scatterpolar(\n                        r=values,\n                        theta=categories,\n                        fill='toself',\n                        name=row['Model']\n                    ))\n                \n                fig_comparison.update_layout(\n                    polar=dict(\n                        radialaxis=dict(\n                            visible=True,\n                            range=[0, 1]\n                        )\n                    ),\n                    title=\"Model Performance Radar Chart\",\n                    height=500\n                )\n                \n                st.plotly_chart(fig_comparison, use_container_width=True)\n    \n    def render_page(self) -> None:\n        \"\"\"Render the complete model performance page.\"\"\"\n        \n        # Model selection\n        selected_model = self.render_model_selection()\n        \n        if selected_model is None:\n            return\n        \n        st.markdown(\"---\")\n        \n        # Training metrics\n        self.render_training_metrics(selected_model)\n        \n        st.markdown(\"---\")\n        \n        # Confusion matrix\n        self.render_confusion_matrix(selected_model)\n        \n        st.markdown(\"---\")\n        \n        # ROC and PR curves\n        self.render_roc_pr_curves(selected_model)\n        \n        st.markdown(\"---\")\n        \n        # Feature importance\n        self.render_feature_importance(selected_model)\n        \n        st.markdown(\"---\")\n        \n        # SHAP analysis\n        self.render_shap_analysis(selected_model)\n        \n        st.markdown(\"---\")\n        \n        # Model comparison\n        self.render_model_comparison()\n\n\ndef render_model_performance_page():\n    \"\"\"Main function to render model performance page.\"\"\"\n    page = ModelPerformancePage()\n    page.render_page()\n\n\nif __name__ == \"__main__\":\n    render_model_performance_page()